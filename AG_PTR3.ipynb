{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1**"
      ],
      "metadata": {
        "id": "ZvrzPXxN89Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 ‚Äî Install dependencies\n",
        "!pip -q install opacus==1.4.0 tqdm pandas matplotlib torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0cbrdow8-Dd",
        "outputId": "ad0ac2b0-6704-4d59-a318-4ccdcb3331ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/224.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 ‚Äî Imports + reproducibility\n",
        "import os, math, random, re, subprocess\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from opacus.accountants import RDPAccountant\n",
        "\n",
        "# Wider alpha grid => avoids \"optimal order is the largest alpha\" warning\n",
        "RDP_ALPHAS = (\n",
        "    [1.01, 1.05] +\n",
        "    [1.1 + 0.1*i for i in range(0, 90)] +      # 1.1..10.0\n",
        "    list(range(11, 64)) + [64, 128, 256, 512]\n",
        ")\n",
        "\n",
        "def seed_all(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaAO_greNS_Q",
        "outputId": "49300779-20f9-4f82-f89b-8c5bed5f087c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 ‚Äî Download Fashion‚ÄëMNIST\n",
        "#torchvision downloads it automatically.\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(len(train_ds), len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf56y3F7NbZO",
        "outputId": "d1ad9359-8f90-4b73-ff71-874a66c9fc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 12.3MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 216kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.91MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 28.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 ‚Äî Make 6000 clients √ó 10 samples/client (FedVRDP-style)\n",
        "def make_cross_device_clients(train_dataset, num_clients=6000, samples_per_client=10, seed=0):\n",
        "    assert num_clients * samples_per_client <= len(train_dataset)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(train_dataset)).tolist()\n",
        "    clients = []\n",
        "    for c in range(num_clients):\n",
        "        clients.append(idx[c*samples_per_client:(c+1)*samples_per_client])\n",
        "    return clients\n",
        "\n",
        "clients = make_cross_device_clients(train_ds, num_clients=6000, samples_per_client=10, seed=0)\n",
        "print(\"num clients:\", len(clients), \"samples/client:\", len(clients[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YelmewFoNjJI",
        "outputId": "601d62ea-7c5e-441d-8512-c4e62f0d4ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num clients: 6000 samples/client: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 ‚Äî Optional ‚Äúpublic anchor set‚Äù (2 samples/class)\n",
        "#This mirrors the idea ‚Äútiny auxiliary data is easy to obtain‚Äù used by Xiang et al\n",
        "#If we want to skip this, set public_idx = [] and don‚Äôt filter.\n",
        "def extract_public_per_class(dataset, per_class=2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    targets = np.array(dataset.targets)\n",
        "    public_idx = []\n",
        "    for k in range(10):\n",
        "        cls_idx = np.where(targets == k)[0]\n",
        "        rng.shuffle(cls_idx)\n",
        "        public_idx.extend(cls_idx[:per_class].tolist())\n",
        "    public_idx = sorted(public_idx)\n",
        "    return public_idx\n",
        "\n",
        "public_idx = extract_public_per_class(train_ds, per_class=2, seed=0)\n",
        "public_loader = DataLoader(Subset(train_ds, public_idx), batch_size=20, shuffle=False)\n",
        "\n",
        "# Remove public samples from clients so they are not used privately\n",
        "public_set = set(public_idx)\n",
        "clients_wo_public = []\n",
        "for cid in range(len(clients)):\n",
        "    filtered = [i for i in clients[cid] if i not in public_set]\n",
        "    clients_wo_public.append(filtered)\n",
        "\n",
        "clients = clients_wo_public\n",
        "print(\"public samples:\", len(public_idx), \"| example client size after removal:\", len(clients[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBTz0cAiUspC",
        "outputId": "82a88968-a091-49b0-adb2-3a331792609f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "public samples: 20 | example client size after removal: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 ‚Äî Model (CNN like FedVRDP description)\n",
        "#FedVRDP describes a CNN with 2 conv layers, maxpool, ReLU, FC(512).\n",
        "class FMNIST_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=0)\n",
        "        self.fc1 = nn.Linear(64*4*4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))       # 28->24\n",
        "        x = F.max_pool2d(x, 2)          # 24->12\n",
        "        x = F.relu(self.conv2(x))       # 12->8\n",
        "        x = F.max_pool2d(x, 2)          # 8->4\n",
        "        x = x.view(x.size(0), -1)       # 64*4*4\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "3e4oXwhgVJOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 ‚Äî Tensor-list utilities (fast, no giant flatten)\n",
        "def model_param_list(model):\n",
        "    return [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "@torch.no_grad()\n",
        "def zero_like_params(model):\n",
        "    return [torch.zeros_like(p) for p in model_param_list(model)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_update_(model, update_list, scale=1.0):\n",
        "    for p, u in zip(model_param_list(model), update_list):\n",
        "        p.data.add_(u, alpha=scale)\n",
        "\n",
        "@torch.no_grad()\n",
        "def l2_norm_list(tlist):\n",
        "    s = None\n",
        "    for t in tlist:\n",
        "        v = (t*t).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return torch.sqrt(s + 1e-12)\n",
        "\n",
        "@torch.no_grad()\n",
        "def dot_list(a_list, b_list):\n",
        "    s = None\n",
        "    for a, b in zip(a_list, b_list):\n",
        "        v = (a*b).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return s\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_scaled_list_(dst, src, alpha):\n",
        "    for d, s in zip(dst, src):\n",
        "        d.add_(s, alpha=alpha)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sub_list(a, b):\n",
        "    return [x - y for x, y in zip(a, b)]"
      ],
      "metadata": {
        "id": "kb2pOiLpVQhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8 ‚Äî Local client training (returns update = local_model ‚àí global_model)\n",
        "def client_update(global_model, client_indices, lr, momentum, local_epochs, batch_size=10):\n",
        "    local_model = deepcopy(global_model)\n",
        "    local_model.train()\n",
        "\n",
        "    loader = DataLoader(Subset(train_ds, client_indices),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    opt = torch.optim.SGD(local_model.parameters(), lr=lr, momentum=momentum)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(local_epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = loss_fn(local_model(x), y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    # delta = local - global\n",
        "    delta = []\n",
        "    for lp, gp in zip(model_param_list(local_model), model_param_list(global_model)):\n",
        "        delta.append((lp.data - gp.data).detach())\n",
        "    return delta"
      ],
      "metadata": {
        "id": "7NaELZRUVTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9 ‚Äî DP accounting helpers (compute noise multiplier for target Œµ)\n",
        "#We will use RDP accountant_toggle: sample rate  ùëû=clients per round/total clients, steps = rounds.\n",
        "def epsilon_from_sigma_dp_sgd(sigma, q, steps, delta):\n",
        "    acc = RDPAccountant(alphas=RDP_ALPHAS)\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma, sample_rate=q)\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def epsilon_from_sigma_two_mech(sigma_sel, sigma_rel, q, steps, delta):\n",
        "    acc = RDPAccountant(alphas=RDP_ALPHAS)\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma_sel, sample_rate=q)  # selection\n",
        "        acc.step(noise_multiplier=sigma_rel, sample_rate=q)  # release\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def find_sigma_for_target_eps_single(target_eps, q, steps, delta, lo=0.1, hi=80.0, iters=50):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_dp_sgd(mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi\n",
        "\n",
        "def find_sigma_for_target_eps_two(target_eps, q, steps, delta, sel_factor=4.0, lo=0.1, hi=200.0, iters=60):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_two_mech(sel_factor*mid, mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi\n",
        "\n",
        "def report_noise_scales(eps_total, q, rounds, delta, C, M, rho, tau, sel_factor):\n",
        "    nm_fedavg = find_sigma_for_target_eps_single(eps_total, q, rounds, delta)\n",
        "    fedavg_mean_noise_std = nm_fedavg * (C / M)\n",
        "\n",
        "    nm_rel = find_sigma_for_target_eps_two(eps_total, q, rounds, delta, sel_factor=sel_factor)\n",
        "    # AG-PTR mean sensitivity is ~ 2*rho/tau; noise std on mean is nm_rel * (2*rho/tau)\n",
        "    agptr_mean_noise_std = nm_rel * (2.0 * rho / tau)\n",
        "\n",
        "    print(f\"Target eps_total = {eps_total}\")\n",
        "    print(f\"q={q:.4f}, rounds={rounds}, delta={delta}\")\n",
        "    print(f\"[DP-FedAvg] nm={nm_fedavg:.4f} -> mean noise std = {fedavg_mean_noise_std:.6g}  (C/M)\")\n",
        "    print(f\"[AG-PTR]   nm_rel={nm_rel:.4f} -> mean noise std = {agptr_mean_noise_std:.6g}  (2rho/tau)\")\n",
        "    print(f\"Noise ratio (AG-PTR / FedAvg) = {agptr_mean_noise_std / fedavg_mean_noise_std:.3f}\")\n",
        "    print(f\"Condition to be tighter: 2rho/tau < C/M  |  2rho/tau={2*rho/tau:.6g}, C/M={C/M:.6g}\")\n"
      ],
      "metadata": {
        "id": "8gCzoHWGVYVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 10 ‚Äî DP‚ÄëFedAvg training loop\n",
        "def train_dp_fedavg(seed, eps_total, delta=1e-5,\n",
        "                    num_clients=6000, clients_per_round=100,\n",
        "                    rounds=180, local_epochs=10, batch_size=10,\n",
        "                    lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                    clip_C=1.0):\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    sigma = find_sigma_for_target_eps_single(eps_total, q, rounds, delta)\n",
        "    achieved_eps = epsilon_from_sigma_dp_sgd(sigma, q, rounds, delta)\n",
        "\n",
        "    mean_noise_std = sigma * (clip_C / clients_per_round)\n",
        "    print(f\"[DP-FedAvg] target eps={eps_total} -> nm={sigma:.4f}, achieved eps‚âà{achieved_eps:.3f}, mean-noise-std‚âà{mean_noise_std:.6g}\")\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"DP-FedAvg eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        sum_update = zero_like_params(model)\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "\n",
        "            norm = l2_norm_list(delta_i)\n",
        "            scale = min(1.0, clip_C / (norm.item() + 1e-12))\n",
        "            add_scaled_list_(sum_update, delta_i, scale)\n",
        "\n",
        "        # add Gaussian noise to the SUM (std = sigma * C)\n",
        "        for j in range(len(sum_update)):\n",
        "            sum_update[j].add_(torch.randn_like(sum_update[j]) * (sigma * clip_C))\n",
        "\n",
        "        avg_update = [u / clients_per_round for u in sum_update]\n",
        "        add_update_(model, avg_update, scale=1.0)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    return acc, achieved_eps, sigma\n"
      ],
      "metadata": {
        "id": "bzo81Sp1VoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 11 ‚Äî Minimal AG‚ÄëPTR training loop (efficient R=2 anchors: ¬±previous_update)\n",
        "#This is a ‚Äúminimal but faithful‚Äù implementation for Experiment 1:\n",
        "def train_ag_ptr(seed, eps_total, delta=1e-5,\n",
        "                 num_clients=6000, clients_per_round=100,\n",
        "                 rounds=180, local_epochs=10, batch_size=10,\n",
        "                 lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                 rho=0.3, tau=60, sel_factor=4.0):\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    sigma_rel = find_sigma_for_target_eps_two(eps_total, q, rounds, delta, sel_factor=sel_factor)\n",
        "    sigma_sel = sel_factor * sigma_rel\n",
        "    achieved_eps = epsilon_from_sigma_two_mech(sigma_sel, sigma_rel, q, rounds, delta)\n",
        "\n",
        "    mean_noise_std = sigma_rel * (2.0 * rho / tau)  # worst-case denominator tau\n",
        "    print(f\"[AG-PTR] target eps={eps_total} -> nm_rel={sigma_rel:.4f}, nm_sel={sigma_sel:.4f}, achieved eps‚âà{achieved_eps:.3f}, mean-noise-std‚âà{mean_noise_std:.6g}\")\n",
        "\n",
        "    anchor = zero_like_params(model)  # DP-safe anchor (start at 0-update)\n",
        "    accept_count = 0\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"AG-PTR eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        # ---- Phase 1: Propose/Test (R=2 anchors: +a, -a) ----\n",
        "        count_pos = 0\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "            s = dot_list(delta_i, anchor).item()\n",
        "            if s >= 0:\n",
        "                count_pos += 1\n",
        "        count_neg = clients_per_round - count_pos\n",
        "\n",
        "        # DP noisy counts (simple approx)\n",
        "        noisy_pos = count_pos + np.random.normal(0.0, sigma_sel)\n",
        "        noisy_neg = count_neg + np.random.normal(0.0, sigma_sel)\n",
        "\n",
        "        select_pos = (noisy_pos >= noisy_neg)\n",
        "        noisy_winner = noisy_pos if select_pos else noisy_neg\n",
        "\n",
        "        if noisy_winner < tau:\n",
        "            continue  # reject round\n",
        "\n",
        "        accept_count += 1\n",
        "        chosen_anchor = anchor if select_pos else [(-a) for a in anchor]\n",
        "\n",
        "        # ---- Phase 2: Release ----\n",
        "        sum_offsets = zero_like_params(model)\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "\n",
        "            s = dot_list(delta_i, anchor).item()\n",
        "            assigned_pos = (s >= 0)\n",
        "            if assigned_pos != select_pos:\n",
        "                continue\n",
        "\n",
        "            offset = sub_list(delta_i, chosen_anchor)\n",
        "            off_norm = l2_norm_list(offset).item()\n",
        "            scale = min(1.0, rho / (off_norm + 1e-12))\n",
        "            add_scaled_list_(sum_offsets, offset, scale)\n",
        "\n",
        "        m_hat = max(tau, int(max(noisy_winner, 0)))\n",
        "\n",
        "        # add DP Gaussian noise to SUM offsets: std = nm_rel * (2*rho)\n",
        "        for j in range(len(sum_offsets)):\n",
        "            sum_offsets[j].add_(torch.randn_like(sum_offsets[j]) * (sigma_rel * (2.0 * rho)))\n",
        "\n",
        "        mean_update = [ca + (so / m_hat) for ca, so in zip(chosen_anchor, sum_offsets)]\n",
        "        add_update_(model, mean_update, scale=1.0)\n",
        "        anchor = [u.detach() for u in mean_update]  # DP-safe post-processing\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    accept_rate = accept_count / rounds\n",
        "    return acc, achieved_eps, (sigma_sel, sigma_rel), accept_rate\n"
      ],
      "metadata": {
        "id": "S36RzmtNV69-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 12 ‚Äî Run the Experiment 1 sweep and plot\n",
        "EPS_GRID = [1, 2, 4]\n",
        "SEEDS = [1, 2, 3]\n",
        "\n",
        "DELTA = 1e-5\n",
        "ROUNDS = 180\n",
        "NUM_CLIENTS = 6000\n",
        "CLIENTS_PER_ROUND = 100\n",
        "LOCAL_EPOCHS = 10\n",
        "BATCH_SIZE = 10\n",
        "LR0 = 0.125\n",
        "LR_DECAY = 0.99\n",
        "MOMENTUM = 0.5\n",
        "\n",
        "CLIP_C = 1.0\n",
        "RHO = 0.3\n",
        "TAU = 60\n",
        "SEL_FACTOR = 4.0\n",
        "\n",
        "# Quick sanity: print noise-on-mean comparison (this is the ‚Äútighter sensitivity‚Äù story)\n",
        "q = CLIENTS_PER_ROUND / NUM_CLIENTS\n",
        "for eps in EPS_GRID:\n",
        "    report_noise_scales(eps, q, ROUNDS, DELTA, C=CLIP_C, M=CLIENTS_PER_ROUND, rho=RHO, tau=TAU, sel_factor=SEL_FACTOR)\n",
        "    print(\"\")\n",
        "\n",
        "rows = []\n",
        "\n",
        "for eps in EPS_GRID:\n",
        "    # DP-FedAvg\n",
        "    accs = []\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, sigma = train_dp_fedavg(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            clip_C=CLIP_C\n",
        "        )\n",
        "        accs.append(acc)\n",
        "    rows.append({\"method\":\"DP-FedAvg\", \"eps_target\":eps, \"acc_mean\":np.mean(accs), \"acc_std\":np.std(accs)})\n",
        "\n",
        "    # AG-PTR\n",
        "    accs = []\n",
        "    acc_rates = []\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, (sig_sel, sig_rel), ar = train_ag_ptr(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            rho=RHO, tau=TAU, sel_factor=SEL_FACTOR\n",
        "        )\n",
        "        accs.append(acc)\n",
        "        acc_rates.append(ar)\n",
        "    rows.append({\"method\":\"AG-PTR\", \"eps_target\":eps, \"acc_mean\":np.mean(accs), \"acc_std\":np.std(accs),\n",
        "                 \"accept_rate_mean\":np.mean(acc_rates)})\n",
        "\n",
        "df_core = pd.DataFrame(rows)\n",
        "print(df_core)\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "core_path = \"results/exp1_core.csv\"\n",
        "df_core.to_csv(core_path, index=False)\n",
        "print(\"Saved:\", core_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZipOT0tWMm5",
        "outputId": "54c3d61a-8b40-4993-b202-3fc65cef6539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=1 -> sigma=1.3142, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:48<00:00,  4.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=1 -> sigma=1.3142, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [13:14<00:00,  4.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=1 -> sigma=1.3142, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [13:01<00:00,  4.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=1 -> sigma_rel=1.3224, sigma_sel=5.2895, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [24:08<00:00,  8.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=1 -> sigma_rel=1.3224, sigma_sel=5.2895, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [24:21<00:00,  8.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=1 -> sigma_rel=1.3224, sigma_sel=5.2895, achieved eps‚âà1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [23:58<00:00,  7.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=2 -> sigma=0.9713, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:33<00:00,  4.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=2 -> sigma=0.9713, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:30<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=2 -> sigma=0.9713, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:30<00:00,  4.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=2 -> sigma_rel=0.9736, sigma_sel=3.8944, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [21:17<00:00,  7.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=2 -> sigma_rel=0.9736, sigma_sel=3.8944, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [22:03<00:00,  7.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=2 -> sigma_rel=0.9736, sigma_sel=3.8944, achieved eps‚âà2.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [23:05<00:00,  7.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=4 -> sigma=0.7472, achieved eps‚âà4.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:24<00:00,  4.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=4 -> sigma=0.7472, achieved eps‚âà4.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:28<00:00,  4.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DP-FedAvg] target eps=4 -> sigma=0.7472, achieved eps‚âà4.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DP-FedAvg eps=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [12:33<00:00,  4.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AG-PTR] target eps=4 -> sigma_rel=0.7481, sigma_sel=2.9922, achieved eps‚âà4.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AG-PTR eps=4:  22%|‚ñà‚ñà‚ñè       | 39/180 [05:27<19:39,  8.36s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 13 ‚Äî Save CSV + plot accuracy vs epsilon\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "csv_path = \"results/exp1_privacy_utility.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n",
        "\n",
        "plt.figure()\n",
        "for method in df[\"method\"].unique():\n",
        "    sub = df[df[\"method\"]==method].sort_values(\"eps_target\")\n",
        "    plt.errorbar(sub[\"eps_target\"], sub[\"acc_mean\"], yerr=sub[\"acc_std\"], marker=\"o\", capsize=4, label=method)\n",
        "\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(r\"Target $\\varepsilon_{\\mathrm{total}}$ (log2 scale)\")\n",
        "plt.ylabel(\"Final test accuracy\")\n",
        "plt.title(\"Experiment 1: Privacy‚Äìutility (Fashion-MNIST, no attack)\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.4)\n",
        "plt.legend()\n",
        "fig_path = \"results/exp1_privacy_utility.png\"\n",
        "plt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", fig_path)"
      ],
      "metadata": {
        "id": "0RY1AlGqWWQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 14 - PDPBRFL. (official repo) ‚Äî ‚Äúno attack‚Äù run\n",
        "# --- Clone + install ---\n",
        "%cd /content\n",
        "!rm -rf Practical-Differentially-Private-and-Byzantine-resilient-Federated\n",
        "!git clone https://github.com/zihangxiang/Practical-Differentially-Private-and-Byzantine-resilient-Federated\n",
        "%cd Practical-Differentially-Private-and-Byzantine-resilient-Federated\n",
        "!pip -q install -r requirements.txt\n",
        "\n",
        "# --- Runner + parser ---\n",
        "import re, subprocess, pandas as pd, numpy as np, os\n",
        "\n",
        "def run_cmd_capture(cmd, log_path):\n",
        "    p = subprocess.run(cmd, text=True, capture_output=True)\n",
        "    out = p.stdout + \"\\n\" + p.stderr\n",
        "    with open(log_path, \"w\") as f:\n",
        "        f.write(out)\n",
        "    return out\n",
        "\n",
        "def parse_test_acc(text):\n",
        "    t = text.lower()\n",
        "    pats = [\n",
        "        r\"test[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "        r\"final[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "        r\"best[^0-9a-z]*test[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "    ]\n",
        "    for pat in pats:\n",
        "        ms = re.findall(pat, t)\n",
        "        if ms:\n",
        "            return float(ms[-1])\n",
        "    return None\n",
        "\n",
        "EPS_GRID = [1, 2, 4]\n",
        "SEEDS = [1, 2, 3]\n",
        "\n",
        "os.makedirs(\"/content/results\", exist_ok=True)\n",
        "rows = []\n",
        "\n",
        "for eps in EPS_GRID:\n",
        "    accs = []\n",
        "    for sd in SEEDS:\n",
        "        log_path = f\"/content/results/xiang_eps{eps}_seed{sd}.log\"\n",
        "        cmd = [\n",
        "            \"python\", \"main.py\",\n",
        "            \"--dataset\", \"fashion\",\n",
        "            \"--att_key\", \"nobyz\",\n",
        "            \"--epsilon\", str(eps),\n",
        "            \"--DP_mode\", \"centralDP\",\n",
        "            \"--seed\", str(sd),\n",
        "            \"--mal_worker_portion\", \"0\",\n",
        "            \"--anti_byz\", \"1\",\n",
        "            \"--non_iid\", \"0\",\n",
        "            \"--start_att\", \"0.0\",\n",
        "            \"--base_lr\", \"0.2\"\n",
        "        ]\n",
        "        out = run_cmd_capture(cmd, log_path)\n",
        "        acc = parse_test_acc(out)\n",
        "        if acc is None:\n",
        "            print(f\"[Xiang] Could not parse accuracy from {log_path}. Showing last 40 lines:\")\n",
        "            print(\"\\n\".join(out.splitlines()[-40:]))\n",
        "            raise RuntimeError(\"Parsing failed; adjust parse_test_acc() patterns to match their output.\")\n",
        "        accs.append(acc)\n",
        "\n",
        "    rows.append({\n",
        "        \"method\": \"Xiang-et-al\",\n",
        "        \"eps_target\": eps,\n",
        "        \"acc_mean\": float(np.mean(accs)),\n",
        "        \"acc_std\": float(np.std(accs)),\n",
        "    })\n",
        "\n",
        "df_x = pd.DataFrame(rows)\n",
        "df_x.to_csv(\"/content/results/exp1_xiang.csv\", index=False)\n",
        "print(df_x)\n",
        "print(\"Saved: /content/results/exp1_xiang.csv\")\n"
      ],
      "metadata": {
        "id": "gG1v5jyvtjQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15 Because the DP‚ÄëBREM repo doesn‚Äôt document exact CLI flags in the README,\n",
        "#this cell first prints args/help, then gives you a runnable scaffold + the same ‚Äúparse into CSV‚Äù logic:\n",
        "%cd /content\n",
        "!rm -rf DP-BREM\n",
        "!git clone https://github.com/xiaolangu/DP-BREM.git\n",
        "%cd DP-BREM\n",
        "\n",
        "# Inspect available CLI args (copy the right flags into the command below)\n",
        "!python main.py --help || true\n",
        "!sed -n '1,220p' args.py\n"
      ],
      "metadata": {
        "id": "Let6E6gTh1sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 16  once you see the real flags (dataset name, epsilon flag, byzantine ratio flag, etc.).\n",
        "# After  edit the cmd = [...] list to match DP‚ÄëBREM‚Äôs argparse, the rest will auto-sweep eps/seeds and write results/exp1_dpbrem.csv\n",
        "import re, subprocess, pandas as pd, numpy as np, os\n",
        "\n",
        "def run_cmd_capture(cmd, log_path):\n",
        "    p = subprocess.run(cmd, text=True, capture_output=True)\n",
        "    out = p.stdout + \"\\n\" + p.stderr\n",
        "    with open(log_path, \"w\") as f:\n",
        "        f.write(out)\n",
        "    return out\n",
        "\n",
        "def parse_test_acc(text):\n",
        "    t = text.lower()\n",
        "    pats = [\n",
        "        r\"test[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "        r\"final[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "        r\"avg[^0-9a-z]*acc(?:uracy)?\\s*[:=]\\s*([0-9]*\\.?[0-9]+)\",\n",
        "    ]\n",
        "    for pat in pats:\n",
        "        ms = re.findall(pat, t)\n",
        "        if ms:\n",
        "            return float(ms[-1])\n",
        "    return None\n",
        "\n",
        "EPS_GRID = [1, 2, 4]\n",
        "SEEDS = [1, 2, 3]\n",
        "\n",
        "os.makedirs(\"/content/results\", exist_ok=True)\n",
        "rows = []\n",
        "\n",
        "for eps in EPS_GRID:\n",
        "    accs = []\n",
        "    for sd in SEEDS:\n",
        "        log_path = f\"/content/results/dpbrem_eps{eps}_seed{sd}.log\"\n",
        "\n",
        "        # TODO: EDIT THESE FLAGS to match DP-BREM's args.py\n",
        "        cmd = [\n",
        "            \"python\", \"main.py\",\n",
        "            \"--dataset\", \"fashionmnist\",     # <-- change if needed (e.g., fmnist)\n",
        "            \"--epsilon\", str(eps),           # <-- change if needed (e.g., --eps)\n",
        "            \"--seed\", str(sd),\n",
        "            \"--byz_frac\", \"0.0\",             # <-- change flag name; set 0 attack\n",
        "            \"--attack\", \"none\",              # <-- change if needed\n",
        "        ]\n",
        "\n",
        "        out = run_cmd_capture(cmd, log_path)\n",
        "        acc = parse_test_acc(out)\n",
        "        if acc is None:\n",
        "            print(f\"[DP-BREM] Could not parse accuracy from {log_path}. Showing last 60 lines:\")\n",
        "            print(\"\\n\".join(out.splitlines()[-60:]))\n",
        "            raise RuntimeError(\"Parsing failed; adjust cmd flags + parse_test_acc() patterns.\")\n",
        "\n",
        "        accs.append(acc)\n",
        "\n",
        "    rows.append({\n",
        "        \"method\": \"DP-BREM\",\n",
        "        \"eps_target\": eps,\n",
        "        \"acc_mean\": float(np.mean(accs)),\n",
        "        \"acc_std\": float(np.std(accs)),\n",
        "    })\n",
        "\n",
        "df_b = pd.DataFrame(rows)\n",
        "df_b.to_csv(\"/content/results/exp1_dpbrem.csv\", index=False)\n",
        "print(df_b)\n",
        "print(\"Saved: /content/results/exp1_dpbrem.csv\")\n"
      ],
      "metadata": {
        "id": "Bd_v1AbriBW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 17 merge\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_if_exists(path):\n",
        "    return pd.read_csv(path) if os.path.exists(path) else None\n",
        "\n",
        "core = load_if_exists(\"results/exp1_core.csv\")\n",
        "xiang = load_if_exists(\"results/exp1_xiang.csv\")\n",
        "brem  = load_if_exists(\"results/exp1_dpbrem.csv\")\n",
        "\n",
        "dfs = [d for d in [core, xiang, brem] if d is not None]\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(\"Methods included:\", sorted(df_all[\"method\"].unique()))\n",
        "display(df_all.sort_values([\"method\",\"eps_target\"]))\n",
        "\n",
        "plt.figure()\n",
        "for method in df_all[\"method\"].unique():\n",
        "    sub = df_all[df_all[\"method\"]==method].sort_values(\"eps_target\")\n",
        "    plt.errorbar(sub[\"eps_target\"], sub[\"acc_mean\"], yerr=sub[\"acc_std\"], marker=\"o\", capsize=4, label=method)\n",
        "\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(r\"Target $\\varepsilon$ (log2 scale)\")\n",
        "plt.ylabel(\"Final test accuracy\")\n",
        "plt.title(\"Experiment 1: Privacy‚Äìutility (Fashion-MNIST, no attack)\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.4)\n",
        "plt.legend()\n",
        "\n",
        "fig_path = \"results/exp1_privacy_utility.png\"\n",
        "plt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", fig_path)\n"
      ],
      "metadata": {
        "id": "cGTwk26BiSAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "AG-PTR",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}