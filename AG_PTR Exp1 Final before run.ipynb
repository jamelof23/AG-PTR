{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1**"
      ],
      "metadata": {
        "id": "ZvrzPXxN89Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 ‚Äî Install dependencies\n",
        "!pip -q install opacus==1.4.0 tqdm pandas matplotlib torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0cbrdow8-Dd",
        "outputId": "f32d19e8-7e9d-4e4e-c0f1-8e467d35e9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/224.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 ‚Äî Imports + reproducibility\n",
        "import os, math, random, re, subprocess\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from opacus.accountants import RDPAccountant\n",
        "\n",
        "# Wider alpha grid => avoids \"optimal order is the largest alpha\" warning\n",
        "RDP_ALPHAS = (\n",
        "    [1.01, 1.05] +\n",
        "    [1.1 + 0.1*i for i in range(0, 90)] +      # 1.1..10.0\n",
        "    list(range(11, 64)) + [64, 128, 256, 512]\n",
        ")\n",
        "\n",
        "def seed_all(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaAO_greNS_Q",
        "outputId": "8d1cb921-4f1a-4137-e2d3-99cebfa5d359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 ‚Äî Download Fashion‚ÄëMNIST\n",
        "#torchvision downloads it automatically.\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(len(train_ds), len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf56y3F7NbZO",
        "outputId": "c970b4d7-79b2-4051-8981-5cbcf77b3fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 10.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 176kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.30MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 19.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 ‚Äî Make 6000 clients √ó 10 samples/client (FedVRDP-style)\n",
        "def make_cross_device_clients(train_dataset, num_clients=6000, samples_per_client=10, seed=0):\n",
        "    assert num_clients * samples_per_client <= len(train_dataset)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(train_dataset)).tolist()\n",
        "    clients = []\n",
        "    for c in range(num_clients):\n",
        "        clients.append(idx[c*samples_per_client:(c+1)*samples_per_client])\n",
        "    return clients\n",
        "\n",
        "clients = make_cross_device_clients(train_ds, num_clients=6000, samples_per_client=10, seed=0)\n",
        "print(\"num clients:\", len(clients), \"samples/client:\", len(clients[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YelmewFoNjJI",
        "outputId": "1814dbf2-295f-4313-f48a-b6e60aa9f56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num clients: 6000 samples/client: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 ‚Äî Optional ‚Äúpublic anchor set‚Äù (2 samples/class)\n",
        "#If we want to skip this, set public_idx = [] and don‚Äôt filter.\n",
        "def extract_public_per_class(dataset, per_class=2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    targets = np.array(dataset.targets)\n",
        "    public_idx = []\n",
        "    for k in range(10):\n",
        "        cls_idx = np.where(targets == k)[0]\n",
        "        rng.shuffle(cls_idx)\n",
        "        public_idx.extend(cls_idx[:per_class].tolist())\n",
        "    public_idx = sorted(public_idx)\n",
        "    return public_idx\n",
        "\n",
        "public_idx = extract_public_per_class(train_ds, per_class=2, seed=0)\n",
        "public_loader = DataLoader(Subset(train_ds, public_idx), batch_size=20, shuffle=False)\n",
        "\n",
        "# Remove public samples from clients so they are not used privately\n",
        "public_set = set(public_idx)\n",
        "clients_wo_public = []\n",
        "for cid in range(len(clients)):\n",
        "    filtered = [i for i in clients[cid] if i not in public_set]\n",
        "    clients_wo_public.append(filtered)\n",
        "\n",
        "clients = clients_wo_public\n",
        "print(\"public samples:\", len(public_idx), \"| example client size after removal:\", len(clients[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBTz0cAiUspC",
        "outputId": "03144bb2-b9e1-46ae-e513-83ad753edc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "public samples: 20 | example client size after removal: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 ‚Äî Model (CNN like FedVRDP description)\n",
        "#FedVRDP describes a CNN with 2 conv layers, maxpool, ReLU, FC(512).\n",
        "class FMNIST_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=0)\n",
        "        self.fc1 = nn.Linear(64*4*4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))       # 28->24\n",
        "        x = F.max_pool2d(x, 2)          # 24->12\n",
        "        x = F.relu(self.conv2(x))       # 12->8\n",
        "        x = F.max_pool2d(x, 2)          # 8->4\n",
        "        x = x.view(x.size(0), -1)       # 64*4*4\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "3e4oXwhgVJOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 ‚Äî Tensor-list utilities (fast, no giant flatten)\n",
        "def model_param_list(model):\n",
        "    return [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "@torch.no_grad()\n",
        "def zero_like_params(model):\n",
        "    return [torch.zeros_like(p) for p in model_param_list(model)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_update_(model, update_list, scale=1.0):\n",
        "    for p, u in zip(model_param_list(model), update_list):\n",
        "        p.data.add_(u, alpha=scale)\n",
        "\n",
        "@torch.no_grad()\n",
        "def l2_norm_list(tlist):\n",
        "    s = None\n",
        "    for t in tlist:\n",
        "        v = (t*t).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return torch.sqrt(s + 1e-12)\n",
        "\n",
        "@torch.no_grad()\n",
        "def dot_list(a_list, b_list):\n",
        "    s = None\n",
        "    for a, b in zip(a_list, b_list):\n",
        "        v = (a*b).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return s\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_scaled_list_(dst, src, alpha):\n",
        "    for d, s in zip(dst, src):\n",
        "        d.add_(s, alpha=alpha)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sub_list(a, b):\n",
        "    return [x - y for x, y in zip(a, b)]"
      ],
      "metadata": {
        "id": "kb2pOiLpVQhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8 ‚Äî Local client training (returns update = local_model ‚àí global_model)\n",
        "def client_update(global_model, client_indices, lr, momentum, local_epochs, batch_size=10):\n",
        "    local_model = deepcopy(global_model)\n",
        "    local_model.train()\n",
        "\n",
        "    loader = DataLoader(Subset(train_ds, client_indices),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    opt = torch.optim.SGD(local_model.parameters(), lr=lr, momentum=momentum)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(local_epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = loss_fn(local_model(x), y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    # delta = local - global\n",
        "    delta = []\n",
        "    for lp, gp in zip(model_param_list(local_model), model_param_list(global_model)):\n",
        "        delta.append((lp.data - gp.data).detach())\n",
        "    return delta"
      ],
      "metadata": {
        "id": "7NaELZRUVTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9 ‚Äî DP accounting helpers (compute noise multiplier for target Œµ)\n",
        "#We will use RDP accountant_toggle: sample rate  ùëû=clients per round/total clients, steps = rounds.\n",
        "#(Opacus 1.4.0 compatible)\n",
        "\n",
        "from opacus.accountants import RDPAccountant\n",
        "\n",
        "def epsilon_from_sigma_dp_sgd(sigma, q, steps, delta):\n",
        "    acc = RDPAccountant()  # <-- no alphas here\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma, sample_rate=q)\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def epsilon_from_sigma_two_mech(sigma_sel, sigma_rel, q, steps, delta):\n",
        "    acc = RDPAccountant()  # <-- no alphas here\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma_sel, sample_rate=q)  # selection\n",
        "        acc.step(noise_multiplier=sigma_rel, sample_rate=q)  # release\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def find_sigma_for_target_eps_single(target_eps, q, steps, delta, lo=0.1, hi=80.0, iters=40):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_dp_sgd(mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi\n",
        "\n",
        "def find_sigma_for_target_eps_two(target_eps, q, steps, delta, sel_factor=4.0, lo=0.1, hi=200.0, iters=40):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_two_mech(sel_factor*mid, mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi\n",
        "\n",
        "def report_noise_scales(target_eps, q, rounds, delta, C, M, rho, tau, sel_factor):\n",
        "    nm_fedavg = find_sigma_for_target_eps_single(target_eps, q, rounds, delta)\n",
        "    nm_rel    = find_sigma_for_target_eps_two(target_eps, q, rounds, delta, sel_factor=sel_factor)\n",
        "    nm_sel    = sel_factor * nm_rel\n",
        "\n",
        "    noise_mean = nm_fedavg * (C / M)     # DP-FedAvg released mean noise std\n",
        "    noise_rel  = nm_rel * (rho / tau)    # AG-PTR released update noise std (conservative, denom>=tau)\n",
        "\n",
        "    achieved = epsilon_from_sigma_two_mech(nm_sel, nm_rel, q, rounds, delta)\n",
        "\n",
        "    print(f\"Target eps_total = {target_eps}\")\n",
        "    print(f\"q={q:.4f}, rounds={rounds}, delta={delta}\")\n",
        "    print(f\"[DP-FedAvg] nm={nm_fedavg:.4f}  -> noise_std_on_mean = {noise_mean:.6g} (C/M)\")\n",
        "    print(f\"[AG-PTR]   nm_sel={nm_sel:.4f}\")\n",
        "    print(f\"[AG-PTR]   nm_rel={nm_rel:.4f} -> noise_std_on_release ‚âà {noise_rel:.6g} (rho/tau)\")\n",
        "    print(f\"[AG-PTR] achieved eps ‚âà {achieved:.3f} (target {target_eps})\")\n",
        "    print(f\"Noise ratio (AG-PTR / FedAvg) = {noise_rel/(noise_mean+1e-12):.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8gCzoHWGVYVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 10 ‚Äî DP‚ÄëFedAvg training loop\n",
        "def train_dp_fedavg(seed, eps_total, delta=1e-5,\n",
        "                    num_clients=6000, clients_per_round=100,\n",
        "                    rounds=180, local_epochs=10, batch_size=10,\n",
        "                    lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                    clip_C=1.0):\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    sigma = find_sigma_for_target_eps_single(eps_total, q, rounds, delta)\n",
        "    achieved_eps = epsilon_from_sigma_dp_sgd(sigma, q, rounds, delta)\n",
        "\n",
        "    mean_noise_std = sigma * (clip_C / clients_per_round)\n",
        "    print(f\"[DP-FedAvg] target eps={eps_total} -> nm={sigma:.4f}, achieved eps‚âà{achieved_eps:.3f}, mean-noise-std‚âà{mean_noise_std:.6g}\")\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"DP-FedAvg eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        sum_update = zero_like_params(model)\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "\n",
        "            norm = l2_norm_list(delta_i)\n",
        "            scale = min(1.0, clip_C / (norm.item() + 1e-12))\n",
        "            add_scaled_list_(sum_update, delta_i, scale)\n",
        "\n",
        "        # add Gaussian noise to the SUM (std = sigma * C)\n",
        "        for j in range(len(sum_update)):\n",
        "            sum_update[j].add_(torch.randn_like(sum_update[j]) * (sigma * clip_C))\n",
        "\n",
        "        avg_update = [u / clients_per_round for u in sum_update]\n",
        "        add_update_(model, avg_update, scale=1.0)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    return acc, achieved_eps, sigma\n"
      ],
      "metadata": {
        "id": "bzo81Sp1VoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 ‚Äî AG‚ÄëPTR (public anchor + zero fallback) with pub_scale\n",
        "\n",
        "def norm_sq_list(tlist):\n",
        "    s = None\n",
        "    for t in tlist:\n",
        "        v = (t*t).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return (s + 1e-12)\n",
        "\n",
        "def public_anchor_update(global_model, public_indices, lr, momentum, public_epochs=1, public_batch_size=None):\n",
        "    \"\"\"\n",
        "    Compute delta_pub = (w_pub - w_global) by training on the public set starting at w_global.\n",
        "    Uses SAME optimizer family (SGD), SAME lr and momentum as clients (as you described in the paper).\n",
        "    Returns list-of-tensors delta_pub on device.\n",
        "    \"\"\"\n",
        "    if public_indices is None or len(public_indices) == 0:\n",
        "        raise ValueError(\"public_indices is empty. Build public_idx first.\")\n",
        "\n",
        "    local_model = deepcopy(global_model).to(device)\n",
        "    local_model.train()\n",
        "\n",
        "    bs = len(public_indices) if (public_batch_size is None) else int(public_batch_size)\n",
        "    loader = DataLoader(Subset(train_ds, public_indices), batch_size=bs, shuffle=True, drop_last=False)\n",
        "\n",
        "    opt = torch.optim.SGD(local_model.parameters(), lr=lr, momentum=momentum)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(int(public_epochs)):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = loss_fn(local_model(x), y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    delta_pub = []\n",
        "    for lp, gp in zip(model_param_list(local_model), model_param_list(global_model)):\n",
        "        delta_pub.append((lp.data - gp.data).detach())\n",
        "    return delta_pub\n",
        "\n",
        "\n",
        "def train_ag_ptr(seed, eps_total, delta=1e-5,\n",
        "                 num_clients=6000, clients_per_round=100,\n",
        "                 rounds=30, local_epochs=2, batch_size=10,\n",
        "                 lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                 rho=0.79, tau=80, sel_factor=2.0,\n",
        "                 public_indices=None, public_epochs=1, public_batch_size=None,\n",
        "                 pub_scale=0.1,\n",
        "                 debug=True):\n",
        "\n",
        "    \"\"\"\n",
        "    AG‚ÄëPTR with A_t = { pub_scale * delta_pub_t , 0 }.\n",
        "\n",
        "    Phase 1: DP noisy counts (2 anchors), select winner, gate with tau.\n",
        "    Phase 2: anchored mean of rho-clipped offsets + Gaussian noise.\n",
        "\n",
        "    Returns: acc, achieved_eps, (nm_sel, nm_rel), accept_rate, avg_contrib, clip_rate\n",
        "    \"\"\"\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    # Two Gaussian mechanisms per round (selection + release), tied by sel_factor\n",
        "    nm_rel = find_sigma_for_target_eps_two(eps_total, q, rounds, delta, sel_factor=sel_factor)\n",
        "    nm_sel = sel_factor * nm_rel\n",
        "    achieved_eps = epsilon_from_sigma_two_mech(nm_sel, nm_rel, q, rounds, delta)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"[AG-PTR] eps={eps_total} -> nm_rel={nm_rel:.4f}, nm_sel={nm_sel:.4f}, achieved eps‚âà{achieved_eps:.3f} | pub_scale={pub_scale}\")\n",
        "\n",
        "    zero_anchor = zero_like_params(model)\n",
        "\n",
        "    accept_count = 0\n",
        "    total_contrib = 0\n",
        "    clip_count = 0\n",
        "    total_considered = 0\n",
        "\n",
        "    winner_all = np.zeros(2, dtype=int)\n",
        "    winner_acc = np.zeros(2, dtype=int)\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"AG-PTR eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        # --- build public anchor for this round ---\n",
        "        delta_pub = public_anchor_update(\n",
        "            model, public_indices,\n",
        "            lr=lr_t, momentum=momentum,\n",
        "            public_epochs=public_epochs,\n",
        "            public_batch_size=public_batch_size\n",
        "        )\n",
        "        a_pub = [pub_scale * d for d in delta_pub]  # <-- pub_scale is applied here\n",
        "        a_pub_norm = norm_sq_list(a_pub).item()\n",
        "\n",
        "        # --- compute each sampled client update ONCE ---\n",
        "        deltas = []\n",
        "        assign = []  # 0 => public anchor, 1 => zero anchor\n",
        "        n_pub = 0\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "            deltas.append(delta_i)\n",
        "\n",
        "            # distance to zero: ||delta||^2\n",
        "            d0 = norm_sq_list(delta_i).item()\n",
        "\n",
        "            # distance to public: ||delta - a_pub||^2 = ||delta||^2 + ||a_pub||^2 - 2<delta, a_pub>\n",
        "            dp = d0 + a_pub_norm - 2.0 * dot_list(delta_i, a_pub).item()\n",
        "\n",
        "            if dp <= d0:\n",
        "                assign.append(0)\n",
        "                n_pub += 1\n",
        "            else:\n",
        "                assign.append(1)\n",
        "\n",
        "        n_zero = clients_per_round - n_pub\n",
        "\n",
        "        # --- Phase 1: DP noisy counts ---\n",
        "        noisy_pub = n_pub  + np.random.normal(0.0, nm_sel)\n",
        "        noisy_zero = n_zero + np.random.normal(0.0, nm_sel)\n",
        "\n",
        "        r_star = 0 if (noisy_pub >= noisy_zero) else 1\n",
        "        noisy_winner = noisy_pub if (r_star == 0) else noisy_zero\n",
        "\n",
        "        winner_all[r_star] += 1\n",
        "\n",
        "        if noisy_winner < tau:\n",
        "            # reject: no update\n",
        "            continue\n",
        "\n",
        "        accept_count += 1\n",
        "        winner_acc[r_star] += 1\n",
        "\n",
        "        # winner anchor\n",
        "        a_star = a_pub if (r_star == 0) else zero_anchor\n",
        "\n",
        "        # --- Phase 2: release anchored mean ---\n",
        "        sum_offsets = zero_like_params(model)\n",
        "\n",
        "        contrib = 0\n",
        "        for delta_i, r_i in zip(deltas, assign):\n",
        "            if r_i != r_star:\n",
        "                continue\n",
        "\n",
        "            contrib += 1\n",
        "            total_considered += 1\n",
        "\n",
        "            offset = sub_list(delta_i, a_star)\n",
        "            off_norm = l2_norm_list(offset).item()\n",
        "\n",
        "            scale = min(1.0, rho / (off_norm + 1e-12))\n",
        "            if scale < 1.0:\n",
        "                clip_count += 1\n",
        "\n",
        "            add_scaled_list_(sum_offsets, offset, scale)\n",
        "\n",
        "        total_contrib += contrib\n",
        "\n",
        "        m_hat = max(float(tau), float(noisy_winner))  # DP-safe post-processing\n",
        "\n",
        "        # Add Gaussian noise to SUM offsets (std = nm_rel * rho)\n",
        "        for j in range(len(sum_offsets)):\n",
        "            sum_offsets[j].add_(torch.randn_like(sum_offsets[j]) * (nm_rel * rho))\n",
        "\n",
        "        mean_update = [a + (so / m_hat) for a, so in zip(a_star, sum_offsets)]\n",
        "        add_update_(model, mean_update, scale=1.0)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    accept_rate = accept_count / rounds\n",
        "    avg_contrib = (total_contrib / accept_count) if accept_count > 0 else 0.0\n",
        "    clip_rate = (clip_count / total_considered) if total_considered > 0 else 0.0\n",
        "\n",
        "    if debug:\n",
        "        print(\"\\n[AG-PTR debug] Winner histogram (all rounds):\", winner_all, \"sum=\", winner_all.sum())\n",
        "        print(\"[AG-PTR debug] Winner histogram (accepted rounds):\", winner_acc, \"sum=\", winner_acc.sum())\n",
        "\n",
        "    return acc, achieved_eps, (nm_sel, nm_rel), accept_rate, avg_contrib, clip_rate\n"
      ],
      "metadata": {
        "id": "S36RzmtNV69-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 ‚Äî Calibrate pub_scale on a 30‚Äëround pilot (FedAvg vs AG‚ÄëPTR only)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Pilot settings (keep cheap)\n",
        "NUM_CLIENTS = 6000\n",
        "M = 100\n",
        "ROUNDS_PILOT = 30\n",
        "LOCAL_EPOCHS_PILOT = 2\n",
        "DELTA = 1e-5\n",
        "SEED = 1\n",
        "\n",
        "# DP / training settings\n",
        "CLIP_C = 1.0\n",
        "LR0 = 0.125\n",
        "LR_DECAY = 0.99\n",
        "MOMENTUM = 0.5\n",
        "\n",
        "# AG‚ÄëPTR hyperparams (fix these, tune ONLY pub_scale)\n",
        "EPS = 1              # tune at eps=1 (hardest privacy), then reuse for eps=2,4\n",
        "TAU = 80\n",
        "RHO = 0.79\n",
        "SEL_FACTOR = 2.0\n",
        "PUB_EPOCHS = 1\n",
        "PUB_BS = len(public_idx)\n",
        "\n",
        "PUB_SCALE_GRID = [0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "\n",
        "q = M / NUM_CLIENTS\n",
        "print(\"----- noise sanity (release) -----\")\n",
        "print(f\"C/M={CLIP_C/M:.6f} | rho/tau={RHO/TAU:.6f} | ratio={(RHO/TAU)/(CLIP_C/M):.3f}\")\n",
        "\n",
        "print(\"\\n----- DP‚ÄëFedAvg pilot (reference) -----\")\n",
        "acc_fa, _, nm_fa = train_dp_fedavg(\n",
        "    SEED, EPS, delta=DELTA,\n",
        "    num_clients=NUM_CLIENTS, clients_per_round=M,\n",
        "    rounds=ROUNDS_PILOT, local_epochs=LOCAL_EPOCHS_PILOT, batch_size=10,\n",
        "    lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "    clip_C=CLIP_C\n",
        ")\n",
        "print(f\"FedAvg acc={acc_fa:.4f} | nm={nm_fa:.4f}\")\n",
        "\n",
        "rows = []\n",
        "print(\"\\n----- AG‚ÄëPTR pub_scale sweep -----\")\n",
        "for s in PUB_SCALE_GRID:\n",
        "    acc_ag, _, (nm_sel, nm_rel), ar, avg_contrib, clip_rate = train_ag_ptr(\n",
        "        SEED, EPS, delta=DELTA,\n",
        "        num_clients=NUM_CLIENTS, clients_per_round=M,\n",
        "        rounds=ROUNDS_PILOT, local_epochs=LOCAL_EPOCHS_PILOT, batch_size=10,\n",
        "        lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "        rho=RHO, tau=TAU, sel_factor=SEL_FACTOR,\n",
        "        public_indices=public_idx, public_epochs=PUB_EPOCHS, public_batch_size=PUB_BS,\n",
        "        pub_scale=s,\n",
        "        debug=True\n",
        "    )\n",
        "    rows.append({\n",
        "        \"pub_scale\": s,\n",
        "        \"acc\": float(acc_ag),\n",
        "        \"accept\": float(ar),\n",
        "        \"avg_contrib\": float(avg_contrib),\n",
        "        \"clip_rate\": float(clip_rate),\n",
        "        \"nm_rel\": float(nm_rel),\n",
        "        \"nm_sel\": float(nm_sel),\n",
        "    })\n",
        "    print(f\"  pub_scale={s:>4} | acc={acc_ag:.4f} accept={ar:.3f} contrib={avg_contrib:.1f} clip={clip_rate:.3f}\")\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df_sorted = df.sort_values([\"acc\", \"accept\", \"clip_rate\"], ascending=[False, False, True])\n",
        "print(\"\\n=== Sorted (best first) ===\")\n",
        "print(df_sorted)\n",
        "\n",
        "best = df_sorted.iloc[0].to_dict()\n",
        "print(\"\\nBEST pub_scale (pilot):\", best)\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "df.to_csv(\"results/pilot_pubscale_sweep.csv\", index=False)\n",
        "print(\"Saved: results/pilot_pubscale_sweep.csv\")\n"
      ],
      "metadata": {
        "id": "-jmkPsgvird8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 ‚Äî FULL Experiment 1 sweep + plot (FedAvg vs AG-PTR with tuned pub_scale)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Experiment 1 grid (FULL RUN)\n",
        "# -----------------------------\n",
        "EPS_GRID = [1, 2, 4]\n",
        "SEEDS = [1, 2, 3]\n",
        "\n",
        "DELTA = 1e-5\n",
        "ROUNDS = 180\n",
        "NUM_CLIENTS = 6000\n",
        "CLIENTS_PER_ROUND = 100\n",
        "LOCAL_EPOCHS = 10\n",
        "BATCH_SIZE = 10\n",
        "LR0 = 0.125\n",
        "LR_DECAY = 0.99\n",
        "MOMENTUM = 0.5\n",
        "\n",
        "# DP-FedAvg clip\n",
        "CLIP_C = 1.0\n",
        "\n",
        "# AG-PTR hyperparams (FIXED across eps)\n",
        "TAU = 80\n",
        "RHO = 0.79\n",
        "SEL_FACTOR = 2.0\n",
        "\n",
        "# Public anchor computation\n",
        "PUB_EPOCHS = 1\n",
        "PUB_BS = len(public_idx)\n",
        "\n",
        "# Choose pub_scale (from your pilot sweep)\n",
        "# Option A: If you ran the pilot cell that printed `best`, use:\n",
        "try:\n",
        "    BEST_PUB_SCALE = float(best[\"pub_scale\"])\n",
        "except Exception:\n",
        "    # Option B: manually set it here if `best` doesn't exist\n",
        "    BEST_PUB_SCALE = 0.05  # <-- CHANGE THIS to your chosen value\n",
        "\n",
        "print(\"Using BEST_PUB_SCALE =\", BEST_PUB_SCALE)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for eps in EPS_GRID:\n",
        "    # -----------------------------\n",
        "    # DP-FedAvg (3 seeds)\n",
        "    # -----------------------------\n",
        "    fa_accs = []\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, nm = train_dp_fedavg(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            clip_C=CLIP_C\n",
        "        )\n",
        "        fa_accs.append(acc)\n",
        "\n",
        "    rows.append({\n",
        "        \"method\": \"DP-FedAvg\",\n",
        "        \"eps_target\": eps,\n",
        "        \"acc_mean\": float(np.mean(fa_accs)),\n",
        "        \"acc_std\": float(np.std(fa_accs)),\n",
        "    })\n",
        "\n",
        "    # -----------------------------\n",
        "    # AG-PTR (3 seeds)\n",
        "    # -----------------------------\n",
        "    ag_accs = []\n",
        "    ag_accepts = []\n",
        "    ag_clips = []\n",
        "\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, (nm_sel, nm_rel), ar, avg_contrib, clip_rate = train_ag_ptr(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            rho=RHO, tau=TAU, sel_factor=SEL_FACTOR,\n",
        "            public_indices=public_idx, public_epochs=PUB_EPOCHS, public_batch_size=PUB_BS,\n",
        "            pub_scale=BEST_PUB_SCALE,\n",
        "            debug=False\n",
        "        )\n",
        "        ag_accs.append(acc)\n",
        "        ag_accepts.append(ar)\n",
        "        ag_clips.append(clip_rate)\n",
        "\n",
        "    rows.append({\n",
        "        \"method\": \"AG-PTR\",\n",
        "        \"eps_target\": eps,\n",
        "        \"acc_mean\": float(np.mean(ag_accs)),\n",
        "        \"acc_std\": float(np.std(ag_accs)),\n",
        "        \"accept_rate_mean\": float(np.mean(ag_accepts)),\n",
        "        \"clip_rate_mean\": float(np.mean(ag_clips)),\n",
        "        \"pub_scale\": float(BEST_PUB_SCALE),\n",
        "        \"tau\": float(TAU),\n",
        "        \"rho\": float(RHO),\n",
        "        \"sel_factor\": float(SEL_FACTOR),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df)\n",
        "\n",
        "# -----------------------------\n",
        "# Save CSV\n",
        "# -----------------------------\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "csv_path = \"results/exp1_privacy_utility.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n",
        "\n",
        "# -----------------------------\n",
        "# Plot (accuracy vs epsilon)\n",
        "# -----------------------------\n",
        "plt.figure()\n",
        "for method in df[\"method\"].unique():\n",
        "    sub = df[df[\"method\"] == method].sort_values(\"eps_target\")\n",
        "    plt.errorbar(\n",
        "        sub[\"eps_target\"],\n",
        "        sub[\"acc_mean\"],\n",
        "        yerr=sub[\"acc_std\"],\n",
        "        marker=\"o\",\n",
        "        capsize=4,\n",
        "        label=method\n",
        "    )\n",
        "\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(r\"Target $\\varepsilon_{\\mathrm{total}}$ (log2 scale)\")\n",
        "plt.ylabel(\"Final test accuracy\")\n",
        "plt.title(\"Experiment 1: Privacy‚Äìutility (Fashion-MNIST, no Byzantine)\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.4)\n",
        "plt.legend()\n",
        "\n",
        "png_path_results = \"results/exp1_privacy_utility.png\"\n",
        "plt.savefig(png_path_results, dpi=250, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"Saved:\", png_path_results)\n",
        "\n",
        "# Also save into a figs/ folder to upload to Overleaf easily\n",
        "os.makedirs(\"figs\", exist_ok=True)\n",
        "png_path_figs = \"figs/exp1_privacy_utility.png\"\n",
        "import shutil\n",
        "shutil.copyfile(png_path_results, png_path_figs)\n",
        "print(\"Copied to:\", png_path_figs)\n"
      ],
      "metadata": {
        "id": "cyh2EY1twiLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "AG-PTR",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}