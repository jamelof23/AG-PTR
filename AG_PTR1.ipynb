{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/AG-PTR/blob/main/AG_PTR1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1**"
      ],
      "metadata": {
        "id": "ZvrzPXxN89Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 ‚Äî Install dependencies\n",
        "!pip -q install opacus==1.4.0 tqdm pandas matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0cbrdow8-Dd",
        "outputId": "29cadc66-3479-4dd8-f33c-6cf448793d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/224.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 ‚Äî Imports + reproducibility\n",
        "import os, math, random\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from opacus.accountants import RDPAccountant\n",
        "\n",
        "def seed_all(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "id": "kaAO_greNS_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 ‚Äî Download Fashion‚ÄëMNIST\n",
        "#torchvision downloads it automatically.\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_ds = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(len(train_ds), len(test_ds))"
      ],
      "metadata": {
        "id": "zf56y3F7NbZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 ‚Äî Make 6000 clients √ó 10 samples/client (FedVRDP-style)\n",
        "def make_cross_device_clients(train_dataset, num_clients=6000, samples_per_client=10, seed=0):\n",
        "    assert num_clients * samples_per_client <= len(train_dataset)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(train_dataset)).tolist()\n",
        "    clients = []\n",
        "    for c in range(num_clients):\n",
        "        clients.append(idx[c*samples_per_client:(c+1)*samples_per_client])\n",
        "    return clients\n",
        "\n",
        "clients = make_cross_device_clients(train_ds, num_clients=6000, samples_per_client=10, seed=0)\n",
        "print(\"num clients:\", len(clients), \"samples/client:\", len(clients[0]))"
      ],
      "metadata": {
        "id": "YelmewFoNjJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 ‚Äî Optional ‚Äúpublic anchor set‚Äù (2 samples/class)\n",
        "#This mirrors the idea ‚Äútiny auxiliary data is easy to obtain‚Äù used by Xiang et al\n",
        "#If you want to skip this, set public_idx = [] and don‚Äôt filter.\n",
        "def extract_public_per_class(dataset, per_class=2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    targets = np.array(dataset.targets)\n",
        "    public_idx = []\n",
        "    for k in range(10):\n",
        "        cls_idx = np.where(targets == k)[0]\n",
        "        rng.shuffle(cls_idx)\n",
        "        public_idx.extend(cls_idx[:per_class].tolist())\n",
        "    public_idx = sorted(public_idx)\n",
        "    return public_idx\n",
        "\n",
        "public_idx = extract_public_per_class(train_ds, per_class=2, seed=0)\n",
        "public_loader = DataLoader(Subset(train_ds, public_idx), batch_size=20, shuffle=False)\n",
        "\n",
        "# Remove public samples from clients so they are not used privately\n",
        "public_set = set(public_idx)\n",
        "clients_wo_public = []\n",
        "for cid in range(len(clients)):\n",
        "    filtered = [i for i in clients[cid] if i not in public_set]\n",
        "    clients_wo_public.append(filtered)\n",
        "\n",
        "clients = clients_wo_public\n",
        "print(\"public samples:\", len(public_idx), \"| example client size after removal:\", len(clients[0]))"
      ],
      "metadata": {
        "id": "SBTz0cAiUspC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 ‚Äî Model (CNN like FedVRDP description)\n",
        "#FedVRDP describes a CNN with 2 conv layers, maxpool, ReLU, FC(512).\n",
        "class FMNIST_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=0)\n",
        "        self.fc1 = nn.Linear(64*4*4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))       # 28->24\n",
        "        x = F.max_pool2d(x, 2)          # 24->12\n",
        "        x = F.relu(self.conv2(x))       # 12->8\n",
        "        x = F.max_pool2d(x, 2)          # 8->4\n",
        "        x = x.view(x.size(0), -1)       # 64*4*4\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "3e4oXwhgVJOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 ‚Äî Tensor-list utilities (fast, no giant flatten)\n",
        "def model_param_list(model):\n",
        "    return [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "@torch.no_grad()\n",
        "def zero_like_params(model):\n",
        "    return [torch.zeros_like(p) for p in model_param_list(model)]\n",
        "\n",
        "@torch.no_grad()\n",
        "def copy_params_(dst_model, src_model):\n",
        "    for dp, sp in zip(model_param_list(dst_model), model_param_list(src_model)):\n",
        "        dp.data.copy_(sp.data)\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_update_(model, update_list, scale=1.0):\n",
        "    for p, u in zip(model_param_list(model), update_list):\n",
        "        p.data.add_(u, alpha=scale)\n",
        "\n",
        "@torch.no_grad()\n",
        "def l2_norm_list(tlist):\n",
        "    s = None\n",
        "    for t in tlist:\n",
        "        v = (t*t).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return torch.sqrt(s + 1e-12)\n",
        "\n",
        "@torch.no_grad()\n",
        "def dot_list(a_list, b_list):\n",
        "    s = None\n",
        "    for a, b in zip(a_list, b_list):\n",
        "        v = (a*b).sum()\n",
        "        s = v if s is None else s + v\n",
        "    return s\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_scaled_list_(dst, src, alpha):\n",
        "    for d, s in zip(dst, src):\n",
        "        d.add_(s, alpha=alpha)\n",
        "\n",
        "@torch.no_grad()\n",
        "def sub_list(a, b):\n",
        "    return [x - y for x, y in zip(a, b)]"
      ],
      "metadata": {
        "id": "kb2pOiLpVQhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8 ‚Äî Local client training (returns update = local_model ‚àí global_model)\n",
        "def client_update(global_model, client_indices, lr, momentum, local_epochs, batch_size=10):\n",
        "    local_model = deepcopy(global_model)\n",
        "    local_model.train()\n",
        "\n",
        "    loader = DataLoader(Subset(train_ds, client_indices),\n",
        "                        batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    opt = torch.optim.SGD(local_model.parameters(), lr=lr, momentum=momentum)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(local_epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = loss_fn(local_model(x), y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    # delta = local - global\n",
        "    delta = []\n",
        "    for lp, gp in zip(model_param_list(local_model), model_param_list(global_model)):\n",
        "        delta.append((lp.data - gp.data).detach())\n",
        "    return delta"
      ],
      "metadata": {
        "id": "7NaELZRUVTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9 ‚Äî DP accounting helpers (compute noise multiplier for target Œµ)\n",
        "#We will use RDP accountant_toggle: sample rate  ùëû=clients per round/total clients, steps = rounds.\n",
        "def epsilon_from_sigma_dp_sgd(sigma, q, steps, delta):\n",
        "    acc = RDPAccountant()\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma, sample_rate=q)\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def epsilon_from_sigma_two_mech(sigma_sel, sigma_rel, q, steps, delta):\n",
        "    acc = RDPAccountant()\n",
        "    for _ in range(steps):\n",
        "        acc.step(noise_multiplier=sigma_sel, sample_rate=q)  # selection\n",
        "        acc.step(noise_multiplier=sigma_rel, sample_rate=q)  # release\n",
        "    return acc.get_epsilon(delta)\n",
        "\n",
        "def find_sigma_for_target_eps_single(target_eps, q, steps, delta, lo=0.1, hi=50.0, iters=40):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_dp_sgd(mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi\n",
        "\n",
        "def find_sigma_for_target_eps_two(target_eps, q, steps, delta, sel_factor=4.0, lo=0.1, hi=50.0, iters=40):\n",
        "    for _ in range(iters):\n",
        "        mid = (lo + hi) / 2\n",
        "        eps = epsilon_from_sigma_two_mech(sel_factor*mid, mid, q, steps, delta)\n",
        "        if eps > target_eps:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return hi"
      ],
      "metadata": {
        "id": "8gCzoHWGVYVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 10 ‚Äî DP‚ÄëFedAvg training loop\n",
        "def train_dp_fedavg(seed, eps_total, delta=1e-5,\n",
        "                    num_clients=6000, clients_per_round=100,\n",
        "                    rounds=180, local_epochs=10, batch_size=10,\n",
        "                    lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                    clip_C=1.0):\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    sigma = find_sigma_for_target_eps_single(eps_total, q, rounds, delta)\n",
        "    achieved_eps = epsilon_from_sigma_dp_sgd(sigma, q, rounds, delta)\n",
        "    print(f\"[DP-FedAvg] target eps={eps_total} -> sigma={sigma:.4f}, achieved eps‚âà{achieved_eps:.3f}\")\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"DP-FedAvg eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        sum_update = zero_like_params(model)\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "\n",
        "            norm = l2_norm_list(delta_i)\n",
        "            scale = min(1.0, clip_C / (norm.item() + 1e-12))\n",
        "            add_scaled_list_(sum_update, delta_i, scale)\n",
        "\n",
        "        # add Gaussian noise to the SUM (std = sigma * C)\n",
        "        for j in range(len(sum_update)):\n",
        "            sum_update[j].add_(torch.randn_like(sum_update[j]) * (sigma * clip_C))\n",
        "\n",
        "        # average and apply\n",
        "        avg_update = [u / clients_per_round for u in sum_update]\n",
        "        add_update_(model, avg_update, scale=1.0)\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    return acc, achieved_eps, sigma"
      ],
      "metadata": {
        "id": "bzo81Sp1VoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 11 ‚Äî Minimal AG‚ÄëPTR training loop (efficient R=2 anchors: ¬±previous_update)\n",
        "#This is a ‚Äúminimal but faithful‚Äù implementation for Experiment 1:\n",
        "def train_ag_ptr(seed, eps_total, delta=1e-5,\n",
        "                 num_clients=6000, clients_per_round=100,\n",
        "                 rounds=180, local_epochs=10, batch_size=10,\n",
        "                 lr0=0.125, lr_decay=0.99, momentum=0.5,\n",
        "                 rho=0.3, tau=60, sel_factor=4.0):\n",
        "\n",
        "    \"\"\"\n",
        "    rho: anchor-relative clipping radius for offsets\n",
        "    tau: minimum population threshold for release (gate)\n",
        "    sel_factor: sigma_sel = sel_factor * sigma_rel (make selection cost small)\n",
        "    \"\"\"\n",
        "\n",
        "    seed_all(seed)\n",
        "    model = FMNIST_CNN().to(device)\n",
        "    q = clients_per_round / num_clients\n",
        "\n",
        "    sigma_rel = find_sigma_for_target_eps_two(eps_total, q, rounds, delta, sel_factor=sel_factor)\n",
        "    sigma_sel = sel_factor * sigma_rel\n",
        "    achieved_eps = epsilon_from_sigma_two_mech(sigma_sel, sigma_rel, q, rounds, delta)\n",
        "    print(f\"[AG-PTR] target eps={eps_total} -> sigma_rel={sigma_rel:.4f}, sigma_sel={sigma_sel:.4f}, achieved eps‚âà{achieved_eps:.3f}\")\n",
        "\n",
        "    # anchor a_t (list of tensors), initialize as zero update\n",
        "    anchor = zero_like_params(model)\n",
        "\n",
        "    accept_count = 0\n",
        "\n",
        "    for t in tqdm(range(rounds), desc=f\"AG-PTR eps={eps_total}\"):\n",
        "        lr_t = lr0 * (lr_decay ** t)\n",
        "        chosen = np.random.choice(num_clients, size=clients_per_round, replace=False)\n",
        "\n",
        "        # ---- Phase 1: Propose/Test (R=2 anchors: +a, -a) ----\n",
        "        # Count how many clients align with +a vs -a using dot(delta, a)\n",
        "        count_pos = 0\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "            s = dot_list(delta_i, anchor).item()\n",
        "            if s >= 0:\n",
        "                count_pos += 1\n",
        "\n",
        "        count_neg = clients_per_round - count_pos\n",
        "\n",
        "        # DP noisy counts (std = sigma_sel * 1 for counts under add/remove)\n",
        "        noisy_pos = count_pos + np.random.normal(0.0, sigma_sel)\n",
        "        noisy_neg = count_neg + np.random.normal(0.0, sigma_sel)\n",
        "\n",
        "        # select anchor\n",
        "        select_pos = (noisy_pos >= noisy_neg)\n",
        "        noisy_winner = noisy_pos if select_pos else noisy_neg\n",
        "\n",
        "        if noisy_winner < tau:\n",
        "            # reject: no update released\n",
        "            continue\n",
        "\n",
        "        accept_count += 1\n",
        "\n",
        "        chosen_anchor = anchor if select_pos else [(-a) for a in anchor]\n",
        "\n",
        "        # ---- Phase 2: Release anchored mean with clipped offsets + DP noise ----\n",
        "        sum_offsets = zero_like_params(model)\n",
        "        contributors = 0\n",
        "\n",
        "        for cid in chosen:\n",
        "            delta_i = client_update(model, clients[cid], lr=lr_t, momentum=momentum,\n",
        "                                    local_epochs=local_epochs, batch_size=batch_size)\n",
        "\n",
        "            s = dot_list(delta_i, anchor).item()\n",
        "            assigned_pos = (s >= 0)\n",
        "            if assigned_pos != select_pos:\n",
        "                continue\n",
        "\n",
        "            contributors += 1\n",
        "\n",
        "            offset = sub_list(delta_i, chosen_anchor)\n",
        "            off_norm = l2_norm_list(offset).item()\n",
        "            scale = min(1.0, rho / (off_norm + 1e-12))\n",
        "            add_scaled_list_(sum_offsets, offset, scale)\n",
        "\n",
        "        m_hat = max(tau, int(max(noisy_winner, 0)))\n",
        "\n",
        "        # add DP Gaussian noise to SUM offsets (std = sigma_rel * rho)\n",
        "        for j in range(len(sum_offsets)):\n",
        "            sum_offsets[j].add_(torch.randn_like(sum_offsets[j]) * (sigma_rel * rho))\n",
        "\n",
        "        mean_update = [ca + (so / m_hat) for ca, so in zip(chosen_anchor, sum_offsets)]\n",
        "\n",
        "        # apply update\n",
        "        add_update_(model, mean_update, scale=1.0)\n",
        "\n",
        "        # update anchor to be the released update (DP-safe by construction)\n",
        "        anchor = [u.detach() for u in mean_update]\n",
        "\n",
        "    acc = evaluate(model, test_loader)\n",
        "    accept_rate = accept_count / rounds\n",
        "    return acc, achieved_eps, (sigma_sel, sigma_rel), accept_rate"
      ],
      "metadata": {
        "id": "S36RzmtNV69-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 12 ‚Äî Run the Experiment 1 sweep and plot\n",
        "EPS_GRID = [1, 2, 4]\n",
        "SEEDS = [1, 2, 3]   # papers often do multiple seeds\n",
        "\n",
        "# Common settings\n",
        "DELTA = 1e-5\n",
        "ROUNDS = 180\n",
        "NUM_CLIENTS = 6000\n",
        "CLIENTS_PER_ROUND = 100\n",
        "LOCAL_EPOCHS = 10\n",
        "BATCH_SIZE = 10\n",
        "LR0 = 0.125\n",
        "LR_DECAY = 0.99\n",
        "MOMENTUM = 0.5\n",
        "\n",
        "# DP hyperparams (you can tune later)\n",
        "CLIP_C = 1.0      # DP-FedAvg clip\n",
        "RHO = 0.3         # AG-PTR anchor-relative clip for offsets\n",
        "TAU = 60          # threshold\n",
        "SEL_FACTOR = 4.0  # sigma_sel = SEL_FACTOR * sigma_rel\n",
        "\n",
        "rows = []\n",
        "\n",
        "for eps in EPS_GRID:\n",
        "    # DP-FedAvg\n",
        "    accs = []\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, sigma = train_dp_fedavg(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            clip_C=CLIP_C\n",
        "        )\n",
        "        accs.append(acc)\n",
        "    rows.append({\"method\":\"DP-FedAvg\", \"eps_target\":eps, \"acc_mean\":np.mean(accs), \"acc_std\":np.std(accs)})\n",
        "\n",
        "    # AG-PTR\n",
        "    accs = []\n",
        "    acc_rates = []\n",
        "    for sd in SEEDS:\n",
        "        acc, achieved_eps, (sig_sel, sig_rel), ar = train_ag_ptr(\n",
        "            sd, eps, delta=DELTA,\n",
        "            num_clients=NUM_CLIENTS, clients_per_round=CLIENTS_PER_ROUND,\n",
        "            rounds=ROUNDS, local_epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE,\n",
        "            lr0=LR0, lr_decay=LR_DECAY, momentum=MOMENTUM,\n",
        "            rho=RHO, tau=TAU, sel_factor=SEL_FACTOR\n",
        "        )\n",
        "        accs.append(acc)\n",
        "        acc_rates.append(ar)\n",
        "    rows.append({\"method\":\"AG-PTR\", \"eps_target\":eps, \"acc_mean\":np.mean(accs), \"acc_std\":np.std(accs),\n",
        "                 \"accept_rate_mean\":np.mean(acc_rates)})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "9ZipOT0tWMm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 13 ‚Äî Save CSV + plot accuracy vs epsilon\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "csv_path = \"results/exp1_privacy_utility.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n",
        "\n",
        "plt.figure()\n",
        "for method in df[\"method\"].unique():\n",
        "    sub = df[df[\"method\"]==method].sort_values(\"eps_target\")\n",
        "    plt.errorbar(sub[\"eps_target\"], sub[\"acc_mean\"], yerr=sub[\"acc_std\"], marker=\"o\", capsize=4, label=method)\n",
        "\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(r\"Target $\\varepsilon_{\\mathrm{total}}$ (log2 scale)\")\n",
        "plt.ylabel(\"Final test accuracy\")\n",
        "plt.title(\"Experiment 1: Privacy‚Äìutility (Fashion-MNIST, no attack)\")\n",
        "plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.4)\n",
        "plt.legend()\n",
        "fig_path = \"results/exp1_privacy_utility.png\"\n",
        "plt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", fig_path)"
      ],
      "metadata": {
        "id": "0RY1AlGqWWQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 14 - Xiang et al. (official repo) ‚Äî ‚Äúno attack‚Äù run\n",
        "!git clone https://github.com/zihangxiang/Practical-Differentially-Private-and-Byzantine-resilient-Federated.git\n",
        "%cd Practical-Differentially-Private-and-Byzantine-resilient-Federated\n",
        "!pip -q install -r requirements.txt"
      ],
      "metadata": {
        "id": "gG1v5jyvtjQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run (example pattern taken from their README; adapt eps=1,2,4 and set no Byzantine):\n",
        "\n",
        "bash code:\n",
        "\n",
        "python main.py \\\n",
        "  --dataset fashion \\\n",
        "  --att_key nobyz \\\n",
        "  --epsilon 1 \\\n",
        "  --DP_mode centralDP \\\n",
        "  --seed 1 \\\n",
        "  --mal_worker_portion 0 \\\n",
        "  --anti_byz 1 \\\n",
        "  --non_iid 0 \\\n",
        "  --start_att 0.0 \\\n",
        "  --base_lr 0.2\n",
        "\n",
        "Do this for --epsilon 2 and --epsilon 4 (and seeds 1/2/3)\n",
        "\n",
        "How to merge results into your plot:\n",
        "After each run, copy the printed final test accuracy into your exp1_privacy_utility.csv under method \"Xiang-et-al\"."
      ],
      "metadata": {
        "id": "6pnjhJJUtpof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 15 - DP‚ÄëBREM/+ (official repo)\n",
        "%cd /content\n",
        "!git clone https://github.com/xiaolangu/DP-BREM.git\n",
        "%cd DP-BREM"
      ],
      "metadata": {
        "id": "coNbVW6huCKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The repo says you can run main.py and to look at args.py for required parameters.\n",
        "\n",
        "To quickly discover arguments in Colab:\n",
        "\n",
        "bash code:\n",
        "\n",
        "grep -n \"add_argument\" -n args.py | head -n 80\n",
        "\n",
        "Then run with 0 Byzantine, Fashion‚ÄëMNIST, and your target epsilon(s). If the repo uses noise multiplier instead of epsilon, you‚Äôll sweep noise multipliers and report the achieved epsilon using their accounting (that‚Äôs still OK as long as you report achieved eps)."
      ],
      "metadata": {
        "id": "Eka5mov8xvD_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "AG-PTR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}